# 第15章 模型性能的度量

## 15.1 引言

在本章，我们介绍了对（预测）模型整体性能评估的度量方法。

如第2.1节和2.5所提到的（那样），通常，我们可以从统计模型方面来区分解释型和预测型方法。Leo Breiman \(2001b\)指出，对模型（性能）的评估可以从两个方面进行：评估_goodness-of-fit_ \(GoF\)和用_Goodness-of-prediction_ \(GoF\)来评估预测准确率。主要的，GoF通常是用于可解释性的模型中，而GoP则通常是用于预测型模型中的。概括来说，GoF用于研究：构建模型所用的变量预测目标变量的贴合程度是多少？而GoP 用于研究：所构建的模型对于一个新的实例的预测性能如何? 对于一些评估方法，对于GoF和GoP 的解释，通常是基于这些评估方法是基于训练集计算得出的还是基于测试集得出的。

这些模型的评估方法可以用在以下几个方面：

* Model Evaluation: 如果我们想知道模型的表现，例如，模型的预测值的可靠性（我们所期望的error的频率和大小是多少？）？
* Model Comparison: 我们可能会想要对比两个模型的性能，基于此来选择其中一个；
* Out-of-sample and out-of-time comparisons: 我们想评估当模型被用于新的数据上的时候的表现是否会变差；

根据dependent variable的类型（e.g., continuous, binary, categorical, count, etc），我们可以使用不同的性能的评估方法。而且，当新的应用产生时，评估方法也在不同的也在增加。在本章中，我们仅讨论特定的一些评估方法，其中的一些方法将会在接下来的几章中陆续介绍。同时，针对dependent variables我们也只考虑连续性（包括count）和类别型（包括binary）的两种情况。







